{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read yelp.csv into a DataFrame\n",
    "yelp = pd.read_csv('yelp.csv')\n",
    "\n",
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp[\"sentiment\"] = yelp[\"stars\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  sentiment  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0          5  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0          5  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0          4  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0          5  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0          5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp[\"sentiment\"] = yelp[\"sentiment\"].replace([1,2,3,4,5],[\"bad\",\"bad\",\"neutral\",\"good\",\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3526\n",
       "5    3337\n",
       "3    1461\n",
       "2     927\n",
       "1     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.stars.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good       6863\n",
       "bad        1676\n",
       "neutral    1461\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "X = yelp[[\"text\",\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  My wife took me here on my birthday for breakf...      good\n",
       "1  I have no idea why some people give bad review...      good\n",
       "2  love the gyro plate. Rice is so good and I als...      good\n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      good\n",
       "4  General Manager Scott Petello is a good egg!!!...      good"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vectorizer\n",
    "vect = CountVectorizer(ngram_range=(1,2),stop_words='english', min_df = 0.01)\n",
    "#min_df is the percentage of occurance of a word in the entire set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(X[\"text\"])\n",
    "X_features = vect.transform(X[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1010)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In class assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we change the value of min_df value? What is this argument controlling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '10',\n",
       " '100',\n",
       " '11',\n",
       " '12',\n",
       " '15',\n",
       " '20',\n",
       " '25',\n",
       " '30',\n",
       " '40',\n",
       " '50',\n",
       " '99',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'admit',\n",
       " 'afternoon',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ahead',\n",
       " 'amazing',\n",
       " 'ambiance',\n",
       " 'american',\n",
       " 'apparently',\n",
       " 'appetizer',\n",
       " 'appetizers',\n",
       " 'appreciate',\n",
       " 'area',\n",
       " 'aren',\n",
       " 'arizona',\n",
       " 'arrived',\n",
       " 'art',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'ass',\n",
       " 'ate',\n",
       " 'atmosphere',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'authentic',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'az',\n",
       " 'baby',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'baked',\n",
       " 'bar',\n",
       " 'barely',\n",
       " 'bars',\n",
       " 'bartender',\n",
       " 'bartenders',\n",
       " 'based',\n",
       " 'basically',\n",
       " 'basil',\n",
       " 'bathroom',\n",
       " 'bbq',\n",
       " 'bean',\n",
       " 'beans',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'beef',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'black',\n",
       " 'bland',\n",
       " 'blue',\n",
       " 'book',\n",
       " 'bottle',\n",
       " 'bought',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boyfriend',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'bring',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'brunch',\n",
       " 'bucks',\n",
       " 'buffet',\n",
       " 'building',\n",
       " 'bun',\n",
       " 'bunch',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'burrito',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'butter',\n",
       " 'buy',\n",
       " 'cafe',\n",
       " 'cake',\n",
       " 'called',\n",
       " 'came',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'carne',\n",
       " 'carry',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'casual',\n",
       " 'center',\n",
       " 'central',\n",
       " 'certainly',\n",
       " 'chain',\n",
       " 'chairs',\n",
       " 'chance',\n",
       " 'chandler',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'charge',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheese',\n",
       " 'chef',\n",
       " 'chicken',\n",
       " 'chili',\n",
       " 'chinese',\n",
       " 'chips',\n",
       " 'chips salsa',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choose',\n",
       " 'chose',\n",
       " 'city',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'club',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'combination',\n",
       " 'combo',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comfortable',\n",
       " 'coming',\n",
       " 'company',\n",
       " 'complaint',\n",
       " 'completely',\n",
       " 'considering',\n",
       " 'conversation',\n",
       " 'cook',\n",
       " 'cooked',\n",
       " 'cool',\n",
       " 'corn',\n",
       " 'corner',\n",
       " 'cost',\n",
       " 'couldn',\n",
       " 'counter',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'covered',\n",
       " 'cozy',\n",
       " 'crab',\n",
       " 'craving',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'creamy',\n",
       " 'crisp',\n",
       " 'crispy',\n",
       " 'crowd',\n",
       " 'crowded',\n",
       " 'crust',\n",
       " 'cuisine',\n",
       " 'cup',\n",
       " 'curry',\n",
       " 'customer',\n",
       " 'customer service',\n",
       " 'customers',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'daily',\n",
       " 'damn',\n",
       " 'dark',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'decent',\n",
       " 'decided',\n",
       " 'decor',\n",
       " 'deep',\n",
       " 'definitely',\n",
       " 'delicious',\n",
       " 'delish',\n",
       " 'desert',\n",
       " 'dessert',\n",
       " 'desserts',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'different',\n",
       " 'dining',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'dirty',\n",
       " 'disappointed',\n",
       " 'dish',\n",
       " 'dishes',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'don know',\n",
       " 'don like',\n",
       " 'don think',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'double',\n",
       " 'downtown',\n",
       " 'dressing',\n",
       " 'drink',\n",
       " 'drinking',\n",
       " 'drinks',\n",
       " 'drive',\n",
       " 'driving',\n",
       " 'dry',\n",
       " 'early',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eaten',\n",
       " 'eating',\n",
       " 'egg',\n",
       " 'eggs',\n",
       " 'employees',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'entire',\n",
       " 'entree',\n",
       " 'entrees',\n",
       " 'environment',\n",
       " 'especially',\n",
       " 'establishment',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'exactly',\n",
       " 'excellent',\n",
       " 'excited',\n",
       " 'expect',\n",
       " 'expectations',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'fabulous',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'fancy',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'fast food',\n",
       " 'fat',\n",
       " 'favorite',\n",
       " 'favorites',\n",
       " 'feel',\n",
       " 'feel like',\n",
       " 'feeling',\n",
       " 'felt',\n",
       " 'felt like',\n",
       " 'filled',\n",
       " 'filling',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'fish',\n",
       " 'fit',\n",
       " 'fix',\n",
       " 'flat',\n",
       " 'flavor',\n",
       " 'flavorful',\n",
       " 'flavors',\n",
       " 'floor',\n",
       " 'folks',\n",
       " 'food',\n",
       " 'food good',\n",
       " 'food great',\n",
       " 'food service',\n",
       " 'foods',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forward',\n",
       " 'free',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'friday night',\n",
       " 'fried',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'friendly staff',\n",
       " 'friends',\n",
       " 'fries',\n",
       " 'frozen',\n",
       " 'fruit',\n",
       " 'fry',\n",
       " 'fun',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'garlic',\n",
       " 'gave',\n",
       " 'generally',\n",
       " 'generous',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'glass',\n",
       " 'glasses',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'good food',\n",
       " 'good place',\n",
       " 'good service',\n",
       " 'good time',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'grab',\n",
       " 'greasy',\n",
       " 'great',\n",
       " 'great food',\n",
       " 'great place',\n",
       " 'great service',\n",
       " 'green',\n",
       " 'greeted',\n",
       " 'grill',\n",
       " 'grilled',\n",
       " 'grocery',\n",
       " 'group',\n",
       " 'guacamole',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'hang',\n",
       " 'happened',\n",
       " 'happy',\n",
       " 'happy hour',\n",
       " 'hard',\n",
       " 'hate',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'head',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heat',\n",
       " 'heaven',\n",
       " 'heavy',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helpful',\n",
       " 'hey',\n",
       " 'high',\n",
       " 'highly',\n",
       " 'highly recommend',\n",
       " 'hit',\n",
       " 'hole',\n",
       " 'home',\n",
       " 'homemade',\n",
       " 'honestly',\n",
       " 'honey',\n",
       " 'hope',\n",
       " 'horrible',\n",
       " 'hostess',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'huge',\n",
       " 'hummus',\n",
       " 'hungry',\n",
       " 'husband',\n",
       " 'ice',\n",
       " 'ice cream',\n",
       " 'iced',\n",
       " 'idea',\n",
       " 'immediately',\n",
       " 'impressed',\n",
       " 'included',\n",
       " 'including',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'ingredients',\n",
       " 'inside',\n",
       " 'instead',\n",
       " 'interesting',\n",
       " 'interior',\n",
       " 'isn',\n",
       " 'issue',\n",
       " 'italian',\n",
       " 'item',\n",
       " 'items',\n",
       " 'job',\n",
       " 'joint',\n",
       " 'just',\n",
       " 'just like',\n",
       " 'just right',\n",
       " 'kept',\n",
       " 'kick',\n",
       " 'kid',\n",
       " 'kids',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kitchen',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowledgeable',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'la',\n",
       " 'lack',\n",
       " 'lady',\n",
       " 'large',\n",
       " 'late',\n",
       " 'later',\n",
       " 'leave',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'lemon',\n",
       " 'let',\n",
       " 'lettuce',\n",
       " 'level',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'like place',\n",
       " 'liked',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'list',\n",
       " 'literally',\n",
       " 'little',\n",
       " 'little bit',\n",
       " 'live',\n",
       " 'lived',\n",
       " 'living',\n",
       " 'll',\n",
       " 'local',\n",
       " 'located',\n",
       " 'location',\n",
       " 'locations',\n",
       " 'long',\n",
       " 'long time',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'looks like',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'loud',\n",
       " 'love',\n",
       " 'love place',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'loves',\n",
       " 'low',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'mac',\n",
       " 'mac cheese',\n",
       " 'main',\n",
       " 'make',\n",
       " 'make sure',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'mall',\n",
       " 'man',\n",
       " 'manager',\n",
       " 'margaritas',\n",
       " 'market',\n",
       " 'matter',\n",
       " 'maybe',\n",
       " 'meal',\n",
       " 'meals',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meat',\n",
       " 'meats',\n",
       " 'mediocre',\n",
       " 'medium',\n",
       " 'meet',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'menu',\n",
       " 'met',\n",
       " 'mexican',\n",
       " 'mexican food',\n",
       " 'middle',\n",
       " 'mind',\n",
       " 'mini',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'missing',\n",
       " 'mix',\n",
       " 'mixed',\n",
       " 'modern',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'mood',\n",
       " 'morning',\n",
       " 'mouth',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'mushrooms',\n",
       " 'music',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'neighborhood',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nights',\n",
       " 'non',\n",
       " 'noodles',\n",
       " 'normal',\n",
       " 'normally',\n",
       " 'north',\n",
       " 'notch',\n",
       " 'note',\n",
       " 'noticed',\n",
       " 'number',\n",
       " 'obviously',\n",
       " 'offer',\n",
       " 'offered',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'oil',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'old town',\n",
       " 'olive',\n",
       " 'ones',\n",
       " 'onion',\n",
       " 'onions',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'opening',\n",
       " 'opinion',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orange',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'ordering',\n",
       " 'orders',\n",
       " 'original',\n",
       " 'outdoor',\n",
       " 'outside',\n",
       " 'outstanding',\n",
       " 'overall',\n",
       " 'overpriced',\n",
       " 'owned',\n",
       " 'owner',\n",
       " 'owners',\n",
       " 'packed',\n",
       " 'paid',\n",
       " 'paper',\n",
       " 'park',\n",
       " 'parking',\n",
       " 'parking lot',\n",
       " 'particular',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'pasta',\n",
       " 'patio',\n",
       " 'patrons',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'peanut',\n",
       " 'people',\n",
       " 'pepper',\n",
       " 'peppers',\n",
       " 'perfect',\n",
       " 'perfectly',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'pho',\n",
       " 'phoenix',\n",
       " 'phone',\n",
       " 'pick',\n",
       " 'picked',\n",
       " 'pie',\n",
       " 'piece',\n",
       " 'pieces',\n",
       " 'pita',\n",
       " 'pizza',\n",
       " 'pizzas',\n",
       " 'place',\n",
       " 'place good',\n",
       " 'place great',\n",
       " 'places',\n",
       " 'plan',\n",
       " 'plate',\n",
       " 'plates',\n",
       " 'play',\n",
       " 'playing',\n",
       " 'pleasant',\n",
       " 'pleased',\n",
       " 'plenty',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'pool',\n",
       " 'poor',\n",
       " 'pork',\n",
       " 'portion',\n",
       " 'portions',\n",
       " 'possible',\n",
       " 'potato',\n",
       " 'potatoes',\n",
       " 'prefer',\n",
       " 'prepared',\n",
       " 'pretty',\n",
       " 'pretty good',\n",
       " 'previous',\n",
       " 'price',\n",
       " 'priced',\n",
       " 'prices',\n",
       " 'pricey',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'products',\n",
       " 'professional',\n",
       " 'pulled',\n",
       " 'quality',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quiet',\n",
       " 'quite',\n",
       " 'rare',\n",
       " 'rating',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'really good',\n",
       " 'really like',\n",
       " 'really nice',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'reasonably',\n",
       " 'reasonably priced',\n",
       " 'received',\n",
       " 'recently',\n",
       " 'recommend',\n",
       " 'recommended',\n",
       " 'red',\n",
       " 'regular',\n",
       " 'remember',\n",
       " 'rest',\n",
       " 'restaurant',\n",
       " 'restaurants',\n",
       " 'return',\n",
       " 'review',\n",
       " 'reviews',\n",
       " 'ribs',\n",
       " 'rice',\n",
       " 'rich',\n",
       " 'right',\n",
       " 'road',\n",
       " 'roasted',\n",
       " 'rock',\n",
       " 'roll',\n",
       " 'rolls',\n",
       " 'room',\n",
       " 'rooms',\n",
       " 'rude',\n",
       " 'run',\n",
       " 'running',\n",
       " 'sad',\n",
       " 'said',\n",
       " 'salad',\n",
       " 'salads',\n",
       " 'salmon',\n",
       " 'salsa',\n",
       " 'salt',\n",
       " 'salty',\n",
       " 'sample',\n",
       " 'sandwich',\n",
       " 'sandwiches',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'sauce',\n",
       " 'sauces',\n",
       " 'sausage',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'scottsdale',\n",
       " 'seafood',\n",
       " 'seasoned',\n",
       " 'seat',\n",
       " 'seated',\n",
       " 'seating',\n",
       " 'seats',\n",
       " 'second',\n",
       " 'section',\n",
       " 'seeing',\n",
       " 'seen',\n",
       " 'selection',\n",
       " 'sell',\n",
       " 'sense',\n",
       " 'seriously',\n",
       " 'serve',\n",
       " 'served',\n",
       " 'server',\n",
       " 'servers',\n",
       " 'service',\n",
       " 'service good',\n",
       " 'service great',\n",
       " 'serving',\n",
       " 'set',\n",
       " 'share',\n",
       " 'shared',\n",
       " 'shop',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'shot',\n",
       " 'showed',\n",
       " 'shrimp',\n",
       " 'sides',\n",
       " 'sign',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'single',\n",
       " 'sit',\n",
       " 'sitting',\n",
       " 'size',\n",
       " 'sized',\n",
       " 'slice',\n",
       " 'slices',\n",
       " 'slightly',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'smell',\n",
       " 'soda',\n",
       " 'soft',\n",
       " 'solid',\n",
       " 'somewhat',\n",
       " 'son',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'soup',\n",
       " 'sour',\n",
       " 'space',\n",
       " 'special',\n",
       " 'specials',\n",
       " 'spend',\n",
       " 'spent',\n",
       " 'spice',\n",
       " 'spicy',\n",
       " 'spinach',\n",
       " 'split',\n",
       " 'sports',\n",
       " 'spot',\n",
       " 'spring',\n",
       " 'staff',\n",
       " 'staff friendly',\n",
       " 'stand',\n",
       " 'standard',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'stay',\n",
       " 'stayed',\n",
       " 'steak',\n",
       " 'stick',\n",
       " 'stop',\n",
       " 'stopped',\n",
       " 'store',\n",
       " 'stores',\n",
       " 'straight',\n",
       " 'street',\n",
       " 'strip',\n",
       " 'strip mall',\n",
       " 'strong',\n",
       " 'stuff',\n",
       " 'stuffed',\n",
       " 'style',\n",
       " 'sugar',\n",
       " 'suggest',\n",
       " 'summer',\n",
       " 'sun',\n",
       " 'sunday',\n",
       " 'super',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surprise',\n",
       " 'surprised',\n",
       " 'sushi',\n",
       " 'sweet',\n",
       " 'sweet potato',\n",
       " 'table',\n",
       " 'tables',\n",
       " 'taco',\n",
       " 'tacos',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tap',\n",
       " 'taste',\n",
       " 'tasted',\n",
       " 'tasted like',\n",
       " 'tastes',\n",
       " 'tasting',\n",
       " 'tasty',\n",
       " 'tea',\n",
       " 'tell',\n",
       " 'tempe',\n",
       " 'tender',\n",
       " 'terrible',\n",
       " 'thai',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tiny',\n",
       " 'tip',\n",
       " 'toast',\n",
       " 'today',\n",
       " 'told',\n",
       " 'tomato',\n",
       " 'tomatoes',\n",
       " 'ton',\n",
       " 'tons',\n",
       " 'took',\n",
       " 'topped',\n",
       " 'toppings',\n",
       " 'tortilla',\n",
       " 'total',\n",
       " 'totally',\n",
       " 'touch',\n",
       " 'town',\n",
       " 'traditional',\n",
       " 'training',\n",
       " 'treat',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'tuna',\n",
       " 'turkey',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'tv',\n",
       " 'twice',\n",
       " 'type',\n",
       " 'typical',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'unique',\n",
       " 'unless',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'usual',\n",
       " 'usually',\n",
       " 'valley',\n",
       " 'value',\n",
       " 'variety',\n",
       " 've',\n",
       " 'vegetables',\n",
       " 'vegetarian',\n",
       " 'veggie',\n",
       " 'veggies',\n",
       " 'vibe',\n",
       " 'view',\n",
       " 'visit',\n",
       " 'visited',\n",
       " 'visiting',\n",
       " 'wait',\n",
       " 'wait staff',\n",
       " 'waited',\n",
       " 'waiter',\n",
       " 'waiting',\n",
       " 'waitress',\n",
       " 'walk',\n",
       " 'walked',\n",
       " 'walking',\n",
       " 'wall',\n",
       " 'walls',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'warm',\n",
       " 'wasn',\n",
       " 'watch',\n",
       " 'watching',\n",
       " 'water',\n",
       " 'way',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'weird',\n",
       " 'went',\n",
       " 'weren',\n",
       " 'west',\n",
       " 'white',\n",
       " 'wide',\n",
       " 'wife',\n",
       " 'wine',\n",
       " 'wings',\n",
       " 'wish',\n",
       " 'won',\n",
       " 'wonderful',\n",
       " 'word',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worse',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'wouldn',\n",
       " 'wow',\n",
       " 'write',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the list of features\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),stop_words='english', max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(X[\"text\"])\n",
    "X_features = vect.transform(X[\"text\"])\n",
    "X_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. how many bi-grams are included in the feature list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_len = [len(x.split(\" \")) for x in features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer service\n",
      "don know\n",
      "feel like\n",
      "food good\n",
      "good food\n",
      "great food\n",
      "great place\n",
      "happy hour\n",
      "ice cream\n",
      "love place\n",
      "pretty good\n",
      "really good\n",
      "service great\n"
     ]
    }
   ],
   "source": [
    "for i in features:\n",
    "    if(len(i.split(\" \")) == 2):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 92, 119, 153, 162, 185, 188, 189, 199, 219, 259, 332, 346, 388],\n",
      "      dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "index = np.where(np.array(feature_len)==2)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: find the words correspodning to length 2 (bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['customer service', 'don know', 'feel like', 'food good',\n",
       "       'good food', 'great food', 'great place', 'happy hour',\n",
       "       'ice cream', 'love place', 'pretty good', 'really good',\n",
       "       'service great'], dtype='<U16')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features)[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. how many reviews mention customer service?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = X_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.index('customer service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[:,92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  13,   19,   55,   61,  113,  148,  155,  184,  348,  355,  380,\n",
       "         559,  567,  662,  687,  714,  720,  857,  928,  973, 1032, 1085,\n",
       "        1090, 1115, 1212, 1252, 1266, 1271, 1292, 1378, 1379, 1394, 1404,\n",
       "        1405, 1494, 1520, 1548, 1558, 1592, 1641, 1666, 1703, 1723, 1777,\n",
       "        1802, 1901, 2002, 2018, 2044, 2069, 2081, 2101, 2212, 2222, 2330,\n",
       "        2444, 2486, 2492, 2510, 2551, 2658, 2665, 2677, 2755, 2758, 2810,\n",
       "        2874, 2882, 2902, 2927, 2947, 2967, 2977, 2979, 3010, 3316, 3357,\n",
       "        3405, 3406, 3426, 3448, 3469, 3545, 3557, 3570, 3581, 3639, 3705,\n",
       "        3716, 3737, 3740, 3777, 3953, 4032, 4057, 4075, 4090, 4137, 4226,\n",
       "        4250, 4265, 4298, 4327, 4416, 4424, 4480, 4540, 4565, 4590, 4605,\n",
       "        4615, 4646, 4743, 4858, 4893, 4923, 4956, 5039, 5072, 5086, 5116,\n",
       "        5214, 5215, 5226, 5281, 5314, 5328, 5491, 5610, 5637, 5699, 5701,\n",
       "        5833, 5869, 5882, 5889, 5896, 5937, 5944, 6017, 6078, 6096, 6102,\n",
       "        6156, 6211, 6318, 6344, 6347, 6391, 6408, 6571, 6615, 6657, 6671,\n",
       "        6725, 6797, 6798, 6799, 6801, 6820, 6881, 6905, 6986, 7007, 7019,\n",
       "        7031, 7035, 7141, 7170, 7245, 7334, 7336, 7346, 7392, 7493, 7545,\n",
       "        7590, 7695, 7740, 7742, 7764, 7789, 7790, 7800, 7894, 7964, 8012,\n",
       "        8053, 8241, 8266, 8281, 8313, 8377, 8389, 8409, 8551, 8590, 8660,\n",
       "        8719, 8735, 8820, 8927, 8965, 8989, 8999, 9006, 9022, 9031, 9044,\n",
       "        9098, 9308, 9344, 9355, 9361, 9371, 9375, 9415, 9472, 9538, 9557,\n",
       "        9597, 9636, 9767, 9773, 9790, 9801, 9872, 9880], dtype=int64),)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(dtm[:,92]>0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They've gotten better and better for me in the time since this review was written. \\n\\nMy last contact with them was a few days ago when I was having trouble redeeming some Groupons on their website. I called customer service and after waiting a few minutes I spoke with a rep who cheerfully booked four separate flights for me, patiently and manually entering my Groupon info for each one.\\n\\nI think the acquisition by Republic has helped them overall. After Republic took over the in-flight cookies started. It still tends to suck if you aren't Ascent club (like just about any budget-centric airline does), but once you get there it's a good value. When I've had to fly Southwest or USAir I've been disappointed in comparison.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.text.iloc[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(dtm[:,92]>0)[0])\n",
    "#228 reviews talk about customer service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation handling while tokenization, removing stopwords etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"applicant should be proficient in Go\",\"need experience in node.js\",\"expert in C & C#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(vect.fit_transform(text).toarray())\n",
    "dtm.columns = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>applicant</th>\n",
       "      <th>experience</th>\n",
       "      <th>expert</th>\n",
       "      <th>js</th>\n",
       "      <th>need</th>\n",
       "      <th>node</th>\n",
       "      <th>proficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   applicant  experience  expert  js  need  node  proficient\n",
       "0          1           0       0   0     0     0           1\n",
       "1          0           1       0   1     1     1           0\n",
       "2          0           0       1   0     0     0           0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'be', 'during', 'herself', 'out', 'across', 'sincere', 'made', 'along', 'above', 'wherein', 'been', 'nothing', 'mostly', 'none', 'serious', 'although', 'fifty', 'for', 'how', 'former', 'full', 'since', 'down', 'too', 'noone', 'two', 'may', 'us', 'hasnt', 'through', 'we', 'toward', 'further', 'itself', 'found', 'both', 'of', 'by', 'side', 'via', 'however', 'was', 'where', 'those', 'became', 'twenty', 'whenever', 'before', 'enough', 'hundred', 'me', 'per', 'she', 'ten', 'go', 'third', 'very', 'i', 'forty', 'formerly', 'would', 'keep', 'her', 'latter', 'fill', 'describe', 'are', 'every', 'mine', 'within', 'cry', 'anything', 'their', 'together', 'onto', 'everyone', 'hence', 'nobody', 'whose', 'but', 'even', 'can', 'between', 'elsewhere', 'first', 'towards', 'give', 'whereafter', 'three', 'call', 'wherever', 'seemed', 'anyone', 'a', 'move', 'each', 'seems', 'sixty', 'something', 'some', 'well', 'because', 'anyway', 'neither', 'still', 'yours', 'becomes', 'less', 'in', 'whereby', 'most', 'him', 'anywhere', 'all', 'eight', 'show', 'than', 'its', 'one', 'everything', 'besides', 'nor', 'here', 'such', 'somewhere', 'themselves', 'what', 'amoungst', 'when', 'somehow', 'there', 'under', 'only', 'six', 'them', 'alone', 'whereas', 'without', 'latterly', 'herein', 'thin', 'could', 'himself', 'therein', 'rather', 'thus', 'with', 'will', 'should', 'against', 'thence', 'beyond', 'beforehand', 'not', 'yourselves', 'or', 'upon', 'whether', 'eg', 'which', 'whereupon', 'much', 'they', 'yourself', 'seem', 'now', 'done', 'system', 'among', 'nine', 'get', 'twelve', 'you', 'amongst', 'back', 'put', 'the', 'moreover', 'already', 'five', 'four', 'others', 'whither', 'nowhere', 'this', 'he', 'take', 'have', 'either', 'my', 'interest', 'then', 'whole', 'whom', 'namely', 'sometimes', 'ie', 'while', 'next', 'part', 'perhaps', 'at', 'to', 'mill', 'meanwhile', 'if', 'otherwise', 'sometime', 'below', 'detail', 'no', 'ltd', 'up', 'own', 'fifteen', 'whoever', 'on', 'his', 'cant', 'afterwards', 'do', 'de', 'ourselves', 'become', 'therefore', 'except', 'name', 'empty', 'who', 'becoming', 'after', 'indeed', 'bill', 'fire', 'as', 'see', 'hereafter', 'why', 'always', 'often', 'hereby', 'whence', 'whatever', 'ever', 'co', 'hers', 'other', 'few', 'ours', 'almost', 'behind', 'thereby', 'hereupon', 'throughout', 'seeming', 'last', 'about', 'once', 'over', 'beside', 'around', 'else', 'top', 'another', 'due', 'anyhow', 'cannot', 'never', 'am', 'least', 're', 'yet', 'must', 'off', 'might', 'that', 'eleven', 'un', 'it', 'find', 'etc', 'until', 'were', 'inc', 'from', 'also', 'someone', 'your', 'many', 'more', 'couldnt', 'myself', 'our', 'bottom', 'though', 'again', 'front', 'nevertheless', 'con', 'thru', 'had', 'so', 'has', 'several', 'into', 'these', 'everywhere', 'an', 'is', 'thereupon', 'please', 'and', 'any', 'same', 'thick', 'being', 'amount', 'thereafter'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    " \n",
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutsom tokenization\n",
    "vect = CountVectorizer(tokenizer=lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&amp;</th>\n",
       "      <th>applicant</th>\n",
       "      <th>be</th>\n",
       "      <th>c</th>\n",
       "      <th>c#</th>\n",
       "      <th>experience</th>\n",
       "      <th>expert</th>\n",
       "      <th>go</th>\n",
       "      <th>in</th>\n",
       "      <th>need</th>\n",
       "      <th>node.js</th>\n",
       "      <th>proficient</th>\n",
       "      <th>should</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   &  applicant  be  c  c#  experience  expert  go  in  need  node.js  \\\n",
       "0  0          1   1  0   0           0       0   1   1     0        0   \n",
       "1  0          0   0  0   0           1       0   0   1     1        1   \n",
       "2  1          0   0  1   1           0       1   0   1     0        0   \n",
       "\n",
       "   proficient  should  \n",
       "0           1       1  \n",
       "1           0       0  \n",
       "2           0       0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = pd.DataFrame(vect.fit_transform(text).toarray())\n",
    "dtm.columns = vect.get_feature_names()\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proficiency in CSharp'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"C#\",\"CSharp\",\"proficiency in c#\",flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Create the DTM again after replacing C# with CSharp and Go with GoLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]=re.sub(\"Go\",\"GoLang\",text[0],flags=re.IGNORECASE)\n",
    "text[2]=re.sub(\"C#\",\"CSharp\",text[2],flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(vect.fit_transform(text).toarray())\n",
    "dtm.columns = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&amp;</th>\n",
       "      <th>applicant</th>\n",
       "      <th>be</th>\n",
       "      <th>c</th>\n",
       "      <th>csharp</th>\n",
       "      <th>experience</th>\n",
       "      <th>expert</th>\n",
       "      <th>golang</th>\n",
       "      <th>in</th>\n",
       "      <th>need</th>\n",
       "      <th>node.js</th>\n",
       "      <th>proficient</th>\n",
       "      <th>should</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   &  applicant  be  c  csharp  experience  expert  golang  in  need  node.js  \\\n",
       "0  0          1   1  0       0           0       0       1   1     0        0   \n",
       "1  0          0   0  0       0           1       0       0   1     1        1   \n",
       "2  1          0   0  1       1           0       1       0   1     0        0   \n",
       "\n",
       "   proficient  should  \n",
       "0           1       1  \n",
       "1           0       0  \n",
       "2           0       0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing inflectional forms via Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"loved the movie\",\"love the acting and the cast\",\"i am loving it\",\"lovely movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(vect.fit_transform(text).toarray())\n",
    "dtm.columns = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acting</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>cast</th>\n",
       "      <th>i</th>\n",
       "      <th>it</th>\n",
       "      <th>love</th>\n",
       "      <th>loved</th>\n",
       "      <th>lovely</th>\n",
       "      <th>loving</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acting  am  and  cast  i  it  love  loved  lovely  loving  movie  the\n",
       "0       0   0    0     0  0   0     0      1       0       0      1    1\n",
       "1       1   0    1     1  0   0     1      0       0       0      0    2\n",
       "2       0   1    0     0  1   1     0      0       0       1      0    0\n",
       "3       0   0    0     0  0   0     0      0       1       0      1    0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LancasterStemmer()\n",
    "sb = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'residential address'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.lemmatize(\"residential address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = PorterStemmer()\n",
    "stem.stem(\"loving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cogniz'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(\"cognizant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stem = []\n",
    "for review in text:\n",
    "    text_stem.append(\" \".join([stem.stem(x) for x in word_tokenize(review)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love the movi', 'love the act and the cast', 'i am love it', 'love movi']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he', 'PRP'), ('works', 'VBZ'), ('in', 'IN'), ('banking', 'NN')]\n",
      "[('i', 'NN'), ('am', 'VBP'), ('banking', 'VBG'), ('on', 'IN'), ('you', 'PRP'), ('to', 'TO'), ('do', 'VB'), ('this', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "# Part of speech may change the meaning of words\n",
    "\n",
    "from nltk import pos_tag\n",
    "print(pos_tag(word_tokenize(\"he works in banking\")))\n",
    "print(pos_tag(word_tokenize(\"i am banking on you to do this\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bank'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"banking\",\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "lemma.lemmatize(\"better\",wordnet.ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_wordnet_pos_tag(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    if tag.startswith(\"R\"):\n",
    "        return wordnet.ADVERB\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"loved\",get_wordnet_pos_tag(\"VBD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'the', 'movie']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemma.lemmatize(x[0],get_wordnet_pos_tag(x[1])) for x in pos_tag(word_tokenize(text[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love the movie'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([lemma.lemmatize(x[0],get_wordnet_pos_tag(x[1])) for x in pos_tag(word_tokenize(text[0]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Using lemmatization normalize all the 3 movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, X_train_orig, X_test_orig = train_test_split(X_features, X[\"sentiment\"], X[\"text\"],\n",
    "                                                                             random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions \n",
    "y_pred_class = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['good', 'good', 'good', ..., 'good', 'good', 'bad'], dtype='<U7')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"actual\":y_test, \"pred\":y_pred_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7364"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>235</td>\n",
       "      <td>139</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>112</td>\n",
       "      <td>1489</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>59</td>\n",
       "      <td>173</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred     bad  good  neutral\n",
       "actual                     \n",
       "bad      235   139       56\n",
       "good     112  1489      120\n",
       "neutral   59   173      117"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(pred_df[\"actual\"],pred_df[\"pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: what does the argument alpha control? How does the model performance change as we change the value of alpha?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_proba = nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bad', 'good', 'neutral'], dtype='<U7')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.09759947e-02, 9.12384877e-01, 6.66391284e-02],\n",
       "       [6.75070771e-05, 9.94569752e-01, 5.36274103e-03],\n",
       "       [1.66133333e-01, 6.85600000e-01, 1.48266667e-01],\n",
       "       ...,\n",
       "       [3.46361230e-02, 9.02208028e-01, 6.31558486e-02],\n",
       "       [4.90033506e-02, 5.13875805e-01, 4.37120844e-01],\n",
       "       [9.99972344e-01, 6.89062534e-06, 2.07657840e-05]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09759947e-02, 6.75070771e-05, 1.66133333e-01, ...,\n",
       "       3.46361230e-02, 4.90033506e-02, 9.99972344e-01])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find top 5 reviews with the highest probability of being \"bad\" reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7878</th>\n",
       "      <td>Great prices and fresh food ...</td>\n",
       "      <td>0.020976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>Delicious kosher food in Scottsdale?  Vegetari...</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>D-scust-ing.</td>\n",
       "      <td>0.166133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>All around great.  Great food, great atmospher...</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>Well it was off to Brio last night for \"date\" ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  probability\n",
       "7878                    Great prices and fresh food ...     0.020976\n",
       "3224  Delicious kosher food in Scottsdale?  Vegetari...     0.000068\n",
       "1919                                       D-scust-ing.     0.166133\n",
       "4432  All around great.  Great food, great atmospher...     0.000135\n",
       "4835  Well it was off to Brio last night for \"date\" ...     1.000000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame({\"text\":X_test_orig, \"probability\":y_pred_proba[:,0]})\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.sort_values(by=\"probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A couple Saturdays ago, went with my mom to get my name added to her accounts should the unthinkable happen. We first went to Chase which took all of ten minutes. The personal banker was great - in-out-done.  Then we headed to her Credit Union at approximately 10:00 AM. They were quite busy, apparently. When we checked in, I heard grumbling from other customers and asked the wait time.  I was told there were two people ahead of us in line and it \"shouldn't be long.\"\n",
      "\n",
      "30 minutes later, my mom went to ask how much longer it would take. Jennfer, who was manning the reception desk, dismissed her rudely in front of a lobby of customers. My mom is as sweet as tea in Tennesee, so to hear her called out in front of a lobby of people as a disruption was upsetting.  45 minutes later, I went up to ask if it would be much longer. Jennifer said curtly, \"We're busy.\" Apparently, on Saturdays, they're busy and one should avoid transacting business there on the weekend. I don't recall being spoken to so rudely at even a fast food restaurant in recent years...\n",
      "\n",
      "Another ten minutes or so went by (as did my deadline to make another appointment I had made), I was told it would be \"two more minutes\", as I had to go through their background check to be a member. They were having a difficult time with my Washington Driver License and the weird characters it has (an asterisk in part of the number). After all was said and done, over an hour later, I was approved to be a member and was successfully put as a signer on my mom's account.\n",
      "\n",
      "I asked for Jennifer's card, as I told her I was not happy with the way she had spoken to me or my mother, and I intended to speak with her manager.  To that I received a reply she did not have one. The woman sitting next to her informed me she was the assistant manager and that they were busy on Saturdays. My biggest beef was this: If it was going to be an hour to speak with someone, tell me. I can make other arrangements or come in during the week. Being told first, \"It shouldn't be long,\" and then, \"We're busy\" is not an excuse. If the business is so overwhelming on Saturdays, perhaps it would be wise to staff more people, extend the hours, or inform people of the wait they're likely to experience because of the queue. Or don't open on Saturdays!\n",
      "\n",
      "As a 20 year veteran of the service industry, I am very disappointed at the level of service Arizona Federal Credit Union provided my mom and me, but also all the other people I watched be dismissed like the Credit Union was doing them a favor. After doing some searching for a new home for mom's money, she can get a much better rate at a (four letter word forthcoming) BANK!\n",
      "\n",
      "I'm normally a big fan of member-owned institutions, but based on the incredibly rude serviced I received from Arizona Federal Credit Union, I'd say go to a Credit Union or Bank where the employees actually read the mission statement that is painted on the wall behind them.\n"
     ]
    }
   ],
   "source": [
    "print(result.text.iloc[0]) \n",
    "#Printing 1st positionand not the 0th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 2 stars is extremely generous. Wanna know why? Read on.\n",
      "\n",
      "My party of 7 and a baby got to Pappadeux's around 6:30. We were told that it would be a 30-minute wait. It ended up being more than an hour. We were standing right in front of the hostess station, yet no one ever came to give us updates on how long it would be for our table. We would go check about every 20 or so minutes. One time we were told that our table was ready. When we went to check on it after about 20 minutes we were told that they were waiting on a busboy to clean it. A few minutes later they were still waiting on a busboy. In what universe does it take a restaurant 30 minutes to find a busboy to clean a table? We watched group after group after group get seated. A group of 7 that came after us was seated before us. We ended up being sat next to them; by the time we were getting our menus, they were finishing up their meals.\n",
      "\n",
      "Once we were finally seated, we waited almost 10 minutes before someone came to our table to introduce himself as our waiter. When he asked how we were doing, we kindly told him that we were a little frustrated because it had taken more than twice as long as we were told, and we had been told twice that they were waiting on a busboy to clean the table. The waiter said that it was a busy night and said that he didn't get paid to deal with unhappy customers. He asked, \"So are you gonna do this or what?\" WHAT?! It was so rude - I don't know how he meant it, but it came across as sounding extremely annoyed. We were appalled and my boyfriend asked to speak to a manager.\n",
      "\n",
      "The manager came over and spoke with my boyfriend. Not sure what was said, but after that things got much better. The manager brought us an appetizer plate on the house. Our food ended up being really good - including the alligator. I got shrimp etoufee, and it was great. The gumbo wasn't so fabulous. Overall, everyone was satisfied with their meal. We got three different drinks, and they basically tasted the same: like cherry Nyquil.\n",
      "\n",
      "The food was good, but it's not always about the food. The service was horrible. Don't think I'd ever go there again. There are too many other places with great food and equally great service.\n"
     ]
    }
   ],
   "source": [
    "print(result.text.iloc[5]) \n",
    "#Printing 1st positionand not the 0th index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2),stop_words='english', max_features=2000)\n",
    "\n",
    "# Build the tfidf vectorizer from the training data (\"fit\"), and apply it \n",
    "# (\"transform\").\n",
    "X_features = vectorizer.fit_transform(X[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD\n",
    "svd_mod = TruncatedSVD(1000)\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_features = svd_mod.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, X_train_orig, X_test_orig = train_test_split(X_features, X[\"sentiment\"], X[\"text\"],\n",
    "                                                                             random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 1000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6884"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\AIML\\\\GLAIML\\\\Resource Materials\\\\Statistical NLP\\\\Day1_snlp'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove.6B.50d.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = glove_model.word_vec(\"cat\")\n",
    "b = glove_model.word_vec(\"bat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50941926, 0.00015790963965300008)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Softwares\\Installed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "X[\"text_clean\"] = [re.sub(\"[^a-zA-Z ]\",\"\",x).lower() for x in X[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'wife',\n",
       " 'took',\n",
       " 'me',\n",
       " 'here',\n",
       " 'on',\n",
       " 'my',\n",
       " 'birthday',\n",
       " 'for',\n",
       " 'breakfast',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'excellent',\n",
       " '',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'was',\n",
       " 'perfect',\n",
       " 'which',\n",
       " 'made',\n",
       " 'sitting',\n",
       " 'outside',\n",
       " 'overlooking',\n",
       " 'their',\n",
       " 'grounds',\n",
       " 'an',\n",
       " 'absolute',\n",
       " 'pleasure',\n",
       " '',\n",
       " 'our',\n",
       " 'waitress',\n",
       " 'was',\n",
       " 'excellent',\n",
       " 'and',\n",
       " 'our',\n",
       " 'food',\n",
       " 'arrived',\n",
       " 'quickly',\n",
       " 'on',\n",
       " 'the',\n",
       " 'semibusy',\n",
       " 'saturday',\n",
       " 'morning',\n",
       " '',\n",
       " 'it',\n",
       " 'looked',\n",
       " 'like',\n",
       " 'the',\n",
       " 'place',\n",
       " 'fills',\n",
       " 'up',\n",
       " 'pretty',\n",
       " 'quickly',\n",
       " 'so',\n",
       " 'the',\n",
       " 'earlier',\n",
       " 'you',\n",
       " 'get',\n",
       " 'here',\n",
       " 'the',\n",
       " 'betterdo',\n",
       " 'yourself',\n",
       " 'a',\n",
       " 'favor',\n",
       " 'and',\n",
       " 'get',\n",
       " 'their',\n",
       " 'bloody',\n",
       " 'mary',\n",
       " '',\n",
       " 'it',\n",
       " 'was',\n",
       " 'phenomenal',\n",
       " 'and',\n",
       " 'simply',\n",
       " 'the',\n",
       " 'best',\n",
       " 'ive',\n",
       " 'ever',\n",
       " 'had',\n",
       " '',\n",
       " 'im',\n",
       " 'pretty',\n",
       " 'sure',\n",
       " 'they',\n",
       " 'only',\n",
       " 'use',\n",
       " 'ingredients',\n",
       " 'from',\n",
       " 'their',\n",
       " 'garden',\n",
       " 'and',\n",
       " 'blend',\n",
       " 'them',\n",
       " 'fresh',\n",
       " 'when',\n",
       " 'you',\n",
       " 'order',\n",
       " 'it',\n",
       " '',\n",
       " 'it',\n",
       " 'was',\n",
       " 'amazingwhile',\n",
       " 'everything',\n",
       " 'on',\n",
       " 'the',\n",
       " 'menu',\n",
       " 'looks',\n",
       " 'excellent',\n",
       " 'i',\n",
       " 'had',\n",
       " 'the',\n",
       " 'white',\n",
       " 'truffle',\n",
       " 'scrambled',\n",
       " 'eggs',\n",
       " 'vegetable',\n",
       " 'skillet',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'tasty',\n",
       " 'and',\n",
       " 'delicious',\n",
       " '',\n",
       " 'it',\n",
       " 'came',\n",
       " 'with',\n",
       " '',\n",
       " 'pieces',\n",
       " 'of',\n",
       " 'their',\n",
       " 'griddled',\n",
       " 'bread',\n",
       " 'with',\n",
       " 'was',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'it',\n",
       " 'absolutely',\n",
       " 'made',\n",
       " 'the',\n",
       " 'meal',\n",
       " 'complete',\n",
       " '',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'toast',\n",
       " 'ive',\n",
       " 'ever',\n",
       " 'hadanyway',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'wait',\n",
       " 'to',\n",
       " 'go',\n",
       " 'back']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = X[\"text_clean\"].iloc[0].split(\" \")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 50)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk = [glove_model.word_vec(x) for x in words if x in glove_model.vocab]\n",
    "np.array(chk).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(chk).mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|                                                             | 1830/10000 [00:00<00:01, 4543.46it/s]F:\\Softwares\\Installed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: Mean of empty slice.\n",
      "  import sys\n",
      "F:\\Softwares\\Installed\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|| 10000/10000 [00:02<00:00, 4461.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "review_vec = np.zeros((X.shape[0],50))\n",
    "for i in tqdm(range(0,X.shape[0])):\n",
    "    words = X[\"text_clean\"].iloc[i].split(\" \")\n",
    "    words = [x.strip() for x in words]\n",
    "    ind_word_vecs = [glove_model.word_vec(x) for x in words if x in glove_model.vocab]\n",
    "    review_vec[i] = np.array(ind_word_vecs).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vec = np.nan_to_num(review_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = review_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
